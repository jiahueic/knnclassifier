{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: K-Nearest Neighbor (10 marks)\n",
    "\n",
    "Student Name: Cheah Jia Huei\n",
    "\n",
    "Student ID: 1078203\n",
    "\n",
    "## General info\n",
    "\n",
    "<b>Due date</b>: Friday, 12 August 2022 5pm\n",
    "\n",
    "<b>Submission method</b>: Canvas submission\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -10% per day up to 5 days (both weekdays and weekends count). Submissions more than 5 days late will not be accepted (resul in a mark of 0).\n",
    "<ul>\n",
    "    <li>one day late, -1.0;</li>\n",
    "    <li>two days late, -2.0;</li>\n",
    "    <li>three days late, -3.0;</li>\n",
    "    <li>four days late, -4.0;</li>\n",
    "    <li>five days late, -5.0;</li>\n",
    "</ul>\n",
    "\n",
    "<b>Extensions</b>: Students who are demonstrably unable to submit a full solution in time due to medical reasons or other trauma, may apply for an extension.  In these cases, you should email <a href=\"mailto:hasti.samadi@unimelb.edu.au\">Hasti Samadi</a> as soon as possible after those circumstances arise. If you attend a GP or other health care service as a result of illness, be sure to provide a Health Professional Report (HPR) form (get it from the Special Consideration section of the Student Portal), you will need this form to be filled out if your illness develops into something that later requires a Special Consideration application to be lodged. You should scan the HPR form and send it with the extension requests.\n",
    "\n",
    "<b>Marks</b>: This assignment will be marked out of 10, and make up 10% of your overall mark for this subject.\n",
    "\n",
    "<b>Materials</b>: See [Using Jupyter Notebook and Python page](https://canvas.lms.unimelb.edu.au/courses/126693/pages/python-and-jupyter-notebooks?module_item_id=3950453) on Canvas (under Modules> Coding Resources) for information on the basic setup required for this class, including an iPython notebook viewer.If your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>.  \n",
    "\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should edit the sections below where requested, but leave the rest of the code as is. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. \n",
    "\n",
    "You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it. We reserve the right to deduct up to 2 marks for unreadable or exessively inefficient code.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via Canvas. Minor changes and clarifications will be announced on the discussion board (Piazza -> Assignments -> A1); we recommend you check it regularly.\n",
    "\n",
    "<b>Academic misconduct</b>: While you may discuss this homework in general terms with other students, it ultimately is still an individual task. Reuse of code or other instances of clear influence will be considered cheating. Please check the <a href=\"https://canvas.lms.unimelb.edu.au/courses/126693/modules#module_734188\">CIS Academic Honesty training</a> for more information. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "Please carefully read and fill out the <b>Authorship Declaration</b> form at the bottom of the page. Failure to fill out this form results in the following deductions: \n",
    "<UL TYPE=”square”>\n",
    "<LI>missing Authorship Declaration at the bottom of the page, -5.0\n",
    "<LI>incomplete or unsigned Authorship Declaration at the bottom of the page, -3.0\n",
    "</UL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this homework, you'll be applying the K-nearest neighbor (KNN) classification algorithm to a real-world machine learning data set. In particular, we will predict the affordability of a car given a diverse set of features, including the make, engine type, style,  and horsepower and other descriptive properties of the car.\n",
    "\n",
    "Firstly, you will read in the dataset into a train and a test set, and you will create two feature sets (Q1). Secondly, you will implement different distance functions (Q2). Thirdly, you will implement one KNN classifier (Q3, Q4) and apply it to the data set using different distance functions and parameter K (Q5). Finally, you will assess the quality of your classifier by comparing its class predictions to the gold standard labels (Q6).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Loading the data (0.5 marks)\n",
    "\n",
    "**Instructions:** For this assignment we will develop a K-Nearest Neighbors (KNN) classifier to predict the \n",
    "affordability of cars. The list of classes is:\n",
    "\n",
    "```\n",
    "cheap\n",
    "affordable\n",
    "expensive\n",
    "very expensive\n",
    "```\n",
    "\n",
    "We use a modified version of the Car data set from the UCI Machine learning repository.\n",
    "\n",
    "The original data can be found here: https://archive.ics.uci.edu/ml/datasets/Automobile\n",
    "\n",
    "The dataset consists of 204 instances. Each instance corresponds to a car which has a unique identifier (X; first field) and is characterized with 24 features as described in the file *car.names* which is provided as part of this assignment.\n",
    "\n",
    "You need to first obtain this dataset, which is on Canvas (assignment 1). The files *car.features* and *car.labels* contain the data we will use in this notebook. Make sure the files are saved in the same folder as this notebook. \n",
    "\n",
    "Both files are in comma-separated value format. The first line in each file is a header, naming each feature (or label).\n",
    "\n",
    "*car.features* contains 204 instances, one line per instance. The first field is the unique instance identifier. The following fields contain the 24 features, as described in the file *car.names*.\n",
    "\n",
    "*car.labels* contains the gold labels (i.e., one of the four classes above), one instance per line. Again, the first field is the instance identifier, and the second field the instance label.\n",
    "\n",
    "*car.names* contains additional explanations about the data set and the features.\n",
    "\n",
    "All feature values are floats, and for Questions 1 through 5, we make the simplifying assumption that all values are indeed real-valued. You may want to revisit this assumption in Question 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Task**: Read the two files  \n",
    "1. create a **training_feature** set (list of features for the first 163 instances in the car.* files) and a **training_label** set (list of labels for the corresponding). \n",
    "2. create a **test_feature** set (list of features of the remaining instances in the car.* files) and a **test_label** set (list of labels for the corresponding). \n",
    "---------\n",
    "- Do **not** shuffle the data.\n",
    "- Do **not** modify feature or label representations. \n",
    "- Features must be represented as floats.\n",
    "--------\n",
    "You may use any Python packages you want, but not change the specified data types (i.e., they should be of type List, and *not* dataframe, dictionary etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"car.features\", 'r').readlines()\n",
    "labels = open(\"car.labels\", 'r').readlines()\n",
    "\n",
    "train_features = []\n",
    "train_labels   = []\n",
    "test_features = []\n",
    "test_labels   = []\n",
    "\n",
    "\n",
    "###########################\n",
    "## YOUR CODE BEGINS HERE\n",
    "###########################\n",
    "# The first line in data is the column headings\n",
    "# need to separate the features by comma delimiters\n",
    "# This is a 2d array\n",
    "# First column is the index in the data set\n",
    "for i in range(1,164):\n",
    "    instance_feature = data[i].split(\",\")\n",
    "    train_features.append(instance_feature[1:])\n",
    "train_labels = labels[1:164]\n",
    "\n",
    "for j in range(164, len(data)):\n",
    "    test_instance_feat = data[j].split(\",\")\n",
    "    test_features.append(test_instance_feat[1:])\n",
    "test_labels = labels[164:]\n",
    "\n",
    "###########################\n",
    "## YOUR CODE ENDS HERE\n",
    "###########################\n",
    "\n",
    "print(\"number of train/test instances:\",len(train_features), len(test_features))\n",
    "print(\"number of train/test features:\",len(train_features[40]), len(test_features[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Distance Functions [1.5 marks]\n",
    "\n",
    "<b>Instructions</b>: Implement the three distance functions specified below. \n",
    "\n",
    "1. Euclidean distance\n",
    "2. Cosine distance\n",
    "3. Chebyshev distance, defined as:\n",
    "    \n",
    "    $d(x,y)=\\max_{i}|x_i-y_i|$\n",
    "    \n",
    "\n",
    "Each distance function takes as input\n",
    "- Two feature vectors (each of type List)\n",
    "\n",
    "and returns as output\n",
    "- The distance between the two feature vectors (float)\n",
    "\n",
    "------------\n",
    "\n",
    "Use <b>only</b> the library imported below, i.e., <b>do not</b> use implementations from any other Python library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(fw1, fw2):\n",
    "    # insert code here\n",
    "    # The two feature vectors should have the same length\n",
    "    # The two feature vectors contain string values\n",
    "    fw1 = np.array(fw1)\n",
    "    fw1 = fw1.astype(float)\n",
    "    fw2 = np.array(fw2)\n",
    "    fw2 = fw2.astype(float)\n",
    "    running_total = 0\n",
    "    for i in range(len(fw1)):\n",
    "        temp = fw1[i] - fw2[i]\n",
    "        running_total += math.pow(temp,2)\n",
    "    distance = math.sqrt(running_total)\n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cosine_distance(fw1, fw2):\n",
    "    # insert code here\n",
    "    fw1 = np.array(fw1)\n",
    "    fw1 = fw1.astype(float)\n",
    "    fw2 = np.array(fw2)\n",
    "    fw2 = fw2.astype(float)\n",
    "    running_total, fw1_squared, fw2_squared = 0, 0, 0\n",
    "    for i in range(len(fw1)):\n",
    "        running_total += fw1[i] * fw2[i]\n",
    "        fw1_squared += math.pow(fw1[i], 2)\n",
    "        fw2_squared += math.pow(fw2[i], 2)\n",
    "\n",
    "    fw1_root = math.sqrt(fw1_squared)\n",
    "    fw2_root = math.sqrt(fw2_squared)\n",
    "    cosine_val = running_total / (fw1_root * fw2_root)\n",
    "    distance = 1 - cosine_val\n",
    "    return distance\n",
    "\n",
    "\n",
    "def chebyshev_distance(fw1, fw2):\n",
    "    # insert code here\n",
    "    # find max diff between the features of the two vectors\n",
    "    # initialise the max value\n",
    "    fw1 = np.array(fw1)\n",
    "    fw1 = fw1.astype(float)\n",
    "    fw2 = np.array(fw2)\n",
    "    fw2 = fw2.astype(float)\n",
    "    max_val = abs(fw1[0] - fw2[0])\n",
    "    for i in range(1,len(fw1)):\n",
    "        if abs(fw1[i] - fw2[i]) > max_val:\n",
    "            max_val = abs(fw1[i] - fw2[i])\n",
    "    distance = max_val\n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################\n",
    "## YOUR CODE ENDS HERE\n",
    "###########################\n",
    "\n",
    "print(round(euclidean_distance(train_features[100],test_features[2]), 5))\n",
    "print(round(chebyshev_distance(train_features[100],test_features[2]), 5))\n",
    "print(round(cosine_distance(train_features[100],test_features[2]), 5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: KNN Classifier [2.0 marks]\n",
    "\n",
    "<b>Instructions</b>: Here, you implement your KNN classifier. It takes as input \n",
    "- training data features\n",
    "- training data labels\n",
    "- test data features\n",
    "- parameter K\n",
    "- distance function(s) based on which nearest neighbors will be identified\n",
    "\n",
    "It returns as output \n",
    "- the predicted labels for the test data\n",
    "\n",
    "**Ties among distances**. If there are more than K instances with the same (smallest) distance value, consider the first K. For example, for K=1 if you have 3 instances (with identifiers i = 3, 12, 54) that all have the same distance to your test instance (e.g., 0.641), the instance with the smallest identifier should be selected as the nearest neighbor (in this case i = 3).\n",
    "\n",
    "**Ties at prediction time.** Ties can also occur at class prediction time when two (or more) classes are supported by the same number of neighbors. In that case choose the class of the 1 nearest neighbor. The \"1 nearest neighbor\" refers only to those classes represented with the maximum support in the neighborhood. E.g., for K = 5, with a neighborhood ordered by distance: {'cheap', 'expensive', 'affordable', expensive', 'affordable'} you would choose the 1 nearest neighbor among {'expensive','affordable'}.\n",
    "\n",
    "-----------\n",
    "\n",
    "**You should implement the classifier from scratch yourself**, i.e., <b> you must not</b> use an existing implementation in any Python library. You may use Python packages (e.g., math, numpy, collections, ...) to help with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "def KNN(train_features, train_labels, test_features, k, dist_fun, weighted=False):\n",
    "    predictions = []\n",
    "    \n",
    "    ###########################\n",
    "    ## Your answer BEGINS HERE\n",
    "    ###########################\n",
    "    \n",
    "    distances = []\n",
    "    # compute the distance of every train_features from evert test feature\n",
    "    # nearest neighbour - which means smallest distance, should sort in ascending order\n",
    "    for i in range(len(test_features)):\n",
    "        distances = []\n",
    "        for j in range(len(train_features)):\n",
    "            distance = dist_fun(test_features[i],train_features[j])\n",
    "            distances.append((j,distance))\n",
    "        # should make prediction after all distances is collected\n",
    "        # the tuples are first sorted by distance, if there is a tie, it is then sorted by index\n",
    "        # (in ascending order)\n",
    "        distances.sort(key=lambda x: x[-1])\n",
    "        # tuple structure \n",
    "        nearest_neigbours = distances[:k]\n",
    "        # create a copy which gets rid of the identifier\n",
    "        top_k = [neighbour[1] for neighbour in nearest_neigbours]\n",
    "        \n",
    "        neighbour_labels = []\n",
    "        # this means the nearest_index_labels entry should have the same index as nearest_neighbours\n",
    "        for neighbour in nearest_neigbours:\n",
    "            neighbour_index = neighbour[0]\n",
    "            neighbour_label = train_labels[neighbour_index].split(\",\")\n",
    "            neighbour_labels.append(neighbour_label)\n",
    "        # create a copy of the list without the index\n",
    "        \n",
    "        labels_copy = [label[-1] for label in neighbour_labels]\n",
    "        \n",
    "        # counter_dict returns something like {'affordable':2,...}\n",
    "        counter_dict = dict()\n",
    "        if not weighted:\n",
    "            counter_dict = Counter(labels_copy)\n",
    "        else:\n",
    "            # create a dictionary to store the weights, keys will be affordable etc\n",
    "            # use a for loop \n",
    "            # nearest neighbours stores the distances of nearest k neighbours\n",
    "            \n",
    "            # print(\"labels copy:\", labels_copy)\n",
    "            def weighted_votes(labels_copy,top_k):\n",
    "                ep = 0.000001\n",
    "                counter_dict = dict()\n",
    "                for m in range(len(top_k)):\n",
    "                    if labels_copy[m] in counter_dict:\n",
    "                        counter_dict[labels_copy[m]] = counter_dict[labels_copy[m]] + 1/(top_k[m] + ep)\n",
    "                    else:\n",
    "                        counter_dict[labels_copy[m]] = 1/(top_k[m] + ep)\n",
    "                return counter_dict\n",
    "            counter_dict = weighted_votes(labels_copy,top_k)\n",
    "        # get the max count from counter_dict\n",
    "        new_value = max(counter_dict,key = counter_dict.get)\n",
    "        max_val = counter_dict[new_value]\n",
    "        counter_list = list(counter_dict.items())\n",
    "        maj_votes = [label[0] for label in counter_list if label[-1] == max_val]\n",
    "\n",
    "        # when there is only one majority neighbour\n",
    "        if len(maj_votes) == 1:\n",
    "            predictions.append(maj_votes[0])\n",
    "\n",
    "        # when there are multiple majority neighbours\n",
    "        elif len(maj_votes) > 1:\n",
    "            # choose one nearest neighbour\n",
    "            def one_nearest_neighbour(maj_votes,labels_copy):\n",
    "                for n in range(len(labels_copy)):\n",
    "                    if labels_copy[n] in maj_votes:\n",
    "                        return labels_copy[n]\n",
    "            ans = one_nearest_neighbour(maj_votes,labels_copy)\n",
    "            predictions.append(ans)\n",
    "    ###########################\n",
    "    ## Your answer ENDS HERE\n",
    "    ###########################\n",
    "        \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Weighted KNN Classifier [1.0 mark]\n",
    "\n",
    "<b>Instructions</b>: Extend your implementation of the KNN classifier in Question 3 to a Weighted KNN classifier. You should change the code in the cell above. Use Inverse Distance as weights:\n",
    "\n",
    "$w_j=\\frac{1}{d_j+\\epsilon}$\n",
    "\n",
    "where\n",
    "\n",
    "- $d_j$ is the distance of of the jth nearest neighbor to the test instance\n",
    "- $\\epsilon=0.000001$\n",
    "\n",
    "Use the Boolean parameter `weighted` to specify the KNN version when calling the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Applying your KNN classifiers to the Car Dataset [0.5 marks]\n",
    "\n",
    "**Using the functions you have implemented above, please**\n",
    "\n",
    "<b> 1. </b>\n",
    "For each of the distance functions you implemented in Question 2, construct (a) Nine majority voting KNN classifiers and (b) Nine weighted KNN classifiers, respectively, with \n",
    "\n",
    "- K=1\n",
    "- K=5\n",
    "- k=20\n",
    "\n",
    "You will obtain a total of 18 (3 distance functions x 3 K values x 2 KNN versions) classifiers.\n",
    "\n",
    "<b> 2. </b>\n",
    "Compute the test accuracy for each model, where the accuracy is the fraction of correctly predicted labels over all predictions. Use the `accuracy_score` function from the `sklearn.metrics` package to obtain your accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "########################\n",
    "# Your code STARTS HERE\n",
    "########################\n",
    "# Nine majority voting KNN classifiers\n",
    "## KNN function signature : def KNN(train_features, train_labels, test_features, k, dist_fun, weighted=False):\n",
    "# distance function signatures: def euclidean_distance(fw1, fw2): def cosine_distance(fw1, fw2): def chebyshev_distance(fw1, fw2):\n",
    "test_labels_copy = list()\n",
    "for i in range(len(test_labels)):\n",
    "    temp = test_labels[i].split(\",\")\n",
    "    # get rid of the identifier in the copy\n",
    "    test_labels_copy.append(temp[-1])\n",
    "pred_euc_1 = KNN(train_features, train_labels, test_features, 1 , euclidean_distance, False)\n",
    "pred_euc_5 = KNN(train_features, train_labels, test_features, 5 , euclidean_distance, False)\n",
    "pred_euc_20 = KNN(train_features, train_labels, test_features, 20 , euclidean_distance, False)\n",
    "\n",
    "pred_cos_1 = KNN(train_features, train_labels, test_features, 1 , cosine_distance, False)\n",
    "pred_cos_5 = KNN(train_features, train_labels, test_features, 5 ,cosine_distance , False)\n",
    "pred_cos_20 = KNN(train_features, train_labels, test_features, 20 ,cosine_distance , False)\n",
    "\n",
    "pred_che_1 = KNN(train_features, train_labels, test_features, 1 , chebyshev_distance, False)\n",
    "pred_che_5 = KNN(train_features, train_labels, test_features, 5 , chebyshev_distance, False)\n",
    "pred_che_20 = KNN(train_features, train_labels, test_features, 20 , chebyshev_distance, False)\n",
    "\n",
    "# Nine weighted KNN classifiers\n",
    "pred_euc_1_w = KNN(train_features, train_labels, test_features, 1 , euclidean_distance, True)\n",
    "pred_euc_5_w = KNN(train_features, train_labels, test_features, 5 , euclidean_distance, True)\n",
    "pred_euc_20_w = KNN(train_features, train_labels, test_features, 20 , euclidean_distance, True)\n",
    "pred_cos_1_w = KNN(train_features, train_labels, test_features, 1 , cosine_distance, True)\n",
    "pred_cos_5_w = KNN(train_features, train_labels, test_features, 5 ,cosine_distance , True)\n",
    "pred_cos_20_w = KNN(train_features, train_labels, test_features, 20 ,cosine_distance , True)\n",
    "\n",
    "pred_che_1_w = KNN(train_features, train_labels, test_features, 1 , chebyshev_distance, True)\n",
    "pred_che_5_w = KNN(train_features, train_labels, test_features, 5 , chebyshev_distance, True)\n",
    "pred_che_20_w = KNN(train_features, train_labels, test_features, 20 , chebyshev_distance, True)\n",
    "\n",
    "accuracy_knn_euc_1 = accuracy_score(test_labels_copy, pred_euc_1)\n",
    "accuracy_knn_euc_5 = accuracy_score(test_labels_copy, pred_euc_5)\n",
    "accuracy_knn_euc_20 = accuracy_score(test_labels_copy, pred_euc_20)\n",
    " \n",
    "accuracy_knn_euc_1_w = accuracy_score(test_labels_copy, pred_euc_1_w)\n",
    "accuracy_knn_euc_5_w = accuracy_score(test_labels_copy, pred_euc_5_w)\n",
    "accuracy_knn_euc_20_w = accuracy_score(test_labels_copy, pred_euc_20_w)\n",
    "\n",
    "accuracy_knn_cos_1 = accuracy_score(test_labels_copy,pred_cos_1)\n",
    "accuracy_knn_cos_5 =  accuracy_score(test_labels_copy,pred_cos_5)\n",
    "accuracy_knn_cos_20 = accuracy_score(test_labels_copy,pred_cos_20)\n",
    "\n",
    "accuracy_knn_cos_1_w = accuracy_score(test_labels_copy,pred_cos_1_w)\n",
    "accuracy_knn_cos_5_w =  accuracy_score(test_labels_copy,pred_cos_5_w)\n",
    "accuracy_knn_cos_20_w = accuracy_score(test_labels_copy,pred_cos_20_w)\n",
    "\n",
    "accuracy_knn_che_1 = accuracy_score(test_labels_copy,pred_che_1)\n",
    "accuracy_knn_che_5 = accuracy_score(test_labels_copy,pred_che_5)\n",
    "accuracy_knn_che_20 = accuracy_score(test_labels_copy,pred_che_20)\n",
    " \n",
    "accuracy_knn_che_1_w = accuracy_score(test_labels_copy,pred_che_1_w)\n",
    "accuracy_knn_che_5_w = accuracy_score(test_labels_copy,pred_che_5_w)\n",
    "accuracy_knn_che_20_w = accuracy_score(test_labels_copy,pred_che_20_w)\n",
    "\n",
    "########################\n",
    "# Your code ENDS HERE\n",
    "########################\n",
    "\n",
    "\n",
    "\n",
    "print(\"Results on the *full* feature set\")\n",
    "\n",
    "print(\"\\neuclidean (majority vote)\")\n",
    "print(\"K=1\", round(accuracy_knn_euc_1, 3))\n",
    "print(\"K=5\", round(accuracy_knn_euc_5, 3))\n",
    "print(\"K=20\", round(accuracy_knn_euc_20, 3))\n",
    "\n",
    "print(\"-----------\\neuclidean (weighted)\")\n",
    "print(\"K=1\", round(accuracy_knn_euc_1_w, 3))\n",
    "print(\"K=5\", round(accuracy_knn_euc_5_w, 3))\n",
    "print(\"K=20\", round(accuracy_knn_euc_20_w, 3))\n",
    "\n",
    "print(\"\\ncosine (majority vote)\")\n",
    "print(\"K=1\", round(accuracy_knn_cos_1, 3))\n",
    "print(\"K=5\", round(accuracy_knn_cos_5, 3))\n",
    "print(\"K=20\", round(accuracy_knn_cos_20, 3))\n",
    "\n",
    "print(\"-----------\\ncosine (weighted)\")\n",
    "print(\"K=1\", round(accuracy_knn_cos_1_w, 3))\n",
    "print(\"K=5\", round(accuracy_knn_cos_5_w, 3))\n",
    "print(\"K=20\", round(accuracy_knn_cos_20_w, 3))\n",
    "\n",
    "print(\"\\nchebyshev (majority vote)\")\n",
    "print(\"K=1\", round(accuracy_knn_che_1, 3))\n",
    "print(\"K=5\", round(accuracy_knn_che_5, 3))\n",
    "print(\"K=20\", round(accuracy_knn_che_20, 3))\n",
    "\n",
    "print(\"-----------\\nchebyshev (weighted)\")\n",
    "print(\"K=1\", round(accuracy_knn_che_1_w, 3))\n",
    "print(\"K=5\", round(accuracy_knn_che_5_w, 3))\n",
    "print(\"K=20\", round(accuracy_knn_che_20_w, 3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Analysis [4.5 marks]\n",
    "\n",
    "1. Consider the following features: make, fuel-type, body-style, and num-of-doors. Assume we intend to use KNN with euclidean distance, for each of the above features, would you change the approach we chose to convert nominal to numeric features? If yes, explain what approach you would select and discuss one benefit and one drawback of your proposed approach.**[0.75 marks]** \n",
    "\n",
    "    \n",
    "2. Consider these two sets of attributes: (curb-weight,engine-size) and (compression-ratio, peak-rpm)\n",
    "\n",
    "    (a) For each set of features, create a scatter plot of data points coloring instances from each class differently. You should produce **two plots** which show the scattered data points colored by class label. Label the x-axis and y-axis. [*N.B. you may use libraries like <a href=\"https://matplotlib.org/stable/tutorials/introductory/usage.html#sphx-glr-tutorials-introductory-usage-py\">matplotlib</a> or <a href=\"https://seaborn.pydata.org/introduction.html\">seaborne</a>*] **[1 mark]**\n",
    "    \n",
    "    (b) Which feature set is more informative in the context of this classification task and why?**[0.5 marks]**\n",
    "    \n",
    "    (c) What do you observe about the relationship between features in each feature set and how did you come to that conclusion?**[0.25 marks]**\n",
    "    \n",
    "    \n",
    "3. Discuss the appropriateness of each of the distance functions for our *car* data set. Where appropriate, explain why you expect them to perform poorly referring to both their mathematical properties and the given feature set. **[0.75 marks]**\n",
    "\n",
    "    \n",
    "\n",
    "4. Does the Weighted KNN outperform the Majority voting version, or vice versa? Hypothesize why (not). **[0.75 mark]**\n",
    "\n",
    "\n",
    "\n",
    "5. Do you think the accuracy is an appropriate evaluation metric for the *car* data set? Why (not)? **[0.5 marks]**\n",
    "\n",
    " \n",
    "\n",
    "<b>Each question should be answered in no more than 3-4 sentences.</b>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1 According to car.names, the approach of mapping catogory name to numbers was used. For the feature of make and body-style, I would change the approach of converting the nominal features to numeric features using one-hot encoding. One benefit of this approach is that it does not introduce artificial ordering of the categories. For example, if we used the original approach, the euclidean distance calculation will be dominated by the make feature if the first feature vector has a make of alfa-romero=1 while the second feature vector has a make of volvo=22. One drawback of the proposed approach is that it increases dimensionality which means the computation time is increased when calculating the euclidean distance for each pair of feature vectors. For example, one-hot encoding will increase the dimensionality of the feature make from 1 to 22 and the dimensionality of the feature body-style from 1 to 5. \n",
    "\n",
    "2\n",
    "\n",
    "*Type code for 2.(a) in the cell below, and answer 2.(b) and 2.(c) below*\n",
    "\n",
    "2b) The feature set of curb-weight and engine-size is more informative. Based on the scatter plots, the data points for each price catogory of cars is more clustered together, meaning there is lower chances of noise when we perform KNN algorithm to achieve the prediction.\n",
    "\n",
    "2c) When curb-weight increases, the engine-size increases. This is because a strong positive correlation value (between 0.67 to 0.85) is observed when pearsonr (from scipy) method is performed on datasets of different labels. When the compression-ratio increases, the peak-rpm decreases. This is because a moderate negative correlation value (between -0.33 to -0.72) was obserbed in cheap, affordable and expensive car labels while a unreliable moderate positive correlation value is observed in very expensive cars (p val > 0.05).\n",
    "\n",
    "3 Cosine distance function is  appropriate as it normalises the magnitude of both feature vectors. Both Euclidean and ChebyShev functions are not appropriate as Euclidean and ChebyShev distances are designed for continuous feature variables whereas 11 out of 24 car features are discrete values mapped from nominal values. Also, ChebyShev is not an all-purpose distance metric, such as Euclidean or Cosine distance, and is used to extract the minimum number of squares to go from one square to another, hence it should be used in scenarios like warehouse logistics instead. \n",
    "\n",
    "4 Weighted KNN does outperform the Majority Voting version. This is because there is small noise in the dataset and there is a small neighbourhood. \n",
    "\n",
    "5 Accuracy is not an appropirate evaluation metric. First, the dataset is not balanced (8 very expensive, 21 expensive, 54 affordable, 81 cheap). Second, we cannot improve the model because we do not know what is the error of the model based on a general accuracy score. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats \n",
    "################################################\n",
    "# Your answer to Question 6 (2) STARTS HERE\n",
    "################################################\n",
    "# should plot train dataset only\n",
    "features = pd.read_csv(\"car.features\")\n",
    "labels = pd.read_csv(\"car.labels\")\n",
    "features = features[:164]\n",
    "labels = labels[:164]\n",
    "merged_data = features.merge(labels)\n",
    "cheap_cars = merged_data.loc[merged_data['price'] == 'cheap']\n",
    "affordable_cars = merged_data.loc[merged_data['price'] == 'affordable']\n",
    "expensive_cars = merged_data.loc[merged_data['price'] =='expensive']\n",
    "very_expensive_cars = merged_data.loc[merged_data['price'] == 'very expensive']\n",
    "plt.figure(0) \n",
    "plt.plot(cheap_cars['curb-weight'].tolist(),cheap_cars['engine-size'].to_list(),'r.')\n",
    "plt.plot(affordable_cars['curb-weight'].tolist(),affordable_cars['engine-size'].to_list(),'b.')\n",
    "plt.plot(expensive_cars['curb-weight'].tolist(),expensive_cars['engine-size'].to_list(),'g.')\n",
    "plt.plot(very_expensive_cars['curb-weight'].tolist(),very_expensive_cars['engine-size'].to_list(),'y.')\n",
    "plt.title('engine-size vs curb-weight')\n",
    "plt.xlabel('curb weight')\n",
    "plt.ylabel('engine size')\n",
    "plt.gca().legend(('cheap_cars','affordable_cars','expensive_cars','very_expensive_cars'))\n",
    "# Performing correlation tests\n",
    "# print(\"Correlation for cheap cars: \",stats.pearsonr(cheap_cars['curb-weight'].tolist(),cheap_cars['engine-size'].to_list()))\n",
    "# print(\"Correlation for affordable cars: \", stats.pearsonr(affordable_cars['curb-weight'].tolist(),affordable_cars['engine-size'].to_list()))\n",
    "# print(\"Correlation for expensive cars: \", stats.pearsonr(expensive_cars['curb-weight'].tolist(),expensive_cars['engine-size'].to_list()))\n",
    "# print(\"Correlation for very expensive cars: \", stats.pearsonr(very_expensive_cars['curb-weight'].tolist(),very_expensive_cars['engine-size'].to_list()))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(cheap_cars['compression-ratio'].tolist(),cheap_cars['peak-rpm'].to_list(),'r.')\n",
    "plt.plot(affordable_cars['compression-ratio'].tolist(),affordable_cars['peak-rpm'].to_list(),'b.')\n",
    "plt.plot(expensive_cars['compression-ratio'].tolist(),expensive_cars['peak-rpm'].to_list(),'g.')\n",
    "plt.plot(very_expensive_cars['compression-ratio'].tolist(),very_expensive_cars['peak-rpm'].to_list(),'y.')\n",
    "plt.title('peak-rpm vs compression-ratio')\n",
    "plt.xlabel('compression ratio')\n",
    "plt.ylabel('peak rpm')\n",
    "plt.gca().legend(('cheap_cars','affordable_cars','expensive_cars','very_expensive_cars'))\n",
    "\n",
    "# Performing correlation tests\n",
    "# print(\"Correlation for cheap cars: \", stats.pearsonr(cheap_cars['compression-ratio'].tolist(),cheap_cars['peak-rpm'].to_list()))\n",
    "# print(\"Correlation for affordable cars: \", stats.pearsonr(affordable_cars['compression-ratio'].tolist(),affordable_cars['peak-rpm'].to_list()))\n",
    "# print(\"Correlation for expensive cars: \", stats.pearsonr(expensive_cars['compression-ratio'].tolist(),expensive_cars['peak-rpm'].to_list()))\n",
    "# print(\"Correlation for very expensive cars: \", stats.pearsonr(very_expensive_cars['compression-ratio'].tolist(),very_expensive_cars['peak-rpm'].to_list()))\n",
    "\n",
    "################################################\n",
    "# Your answer to Question 6 (2) ENDS HERE\n",
    "################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authorship Declaration</b>:\n",
    "\n",
    "   (1) I certify that the program contained in this submission is completely\n",
    "   my own individual work, except where explicitly noted by comments that\n",
    "   provide details otherwise.  I understand that work that has been developed\n",
    "   by another student, or by me in collaboration with other students,\n",
    "   or by non-students as a result of request, solicitation, or payment,\n",
    "   may not be submitted for assessment in this subject.  I understand that\n",
    "   submitting for assessment work developed by or in collaboration with\n",
    "   other students or non-students constitutes Academic Misconduct, and\n",
    "   may be penalized by mark deductions, or by other penalties determined\n",
    "   via the University of Melbourne Academic Honesty Policy, as described\n",
    "   at https://academicintegrity.unimelb.edu.au.\n",
    "\n",
    "   (2) I also certify that I have not provided a copy of this work in either\n",
    "   softcopy or hardcopy or any other form to any other student, and nor will\n",
    "   I do so until after the marks are released. I understand that providing\n",
    "   my work to other students, regardless of my intention or any undertakings\n",
    "   made to me by that other student, is also Academic Misconduct.\n",
    "\n",
    "   (3) I further understand that providing a copy of the assignment\n",
    "   specification to any form of code authoring or assignment tutoring\n",
    "   service, or drawing the attention of others to such services and code\n",
    "   that may have been made available via such a service, may be regarded\n",
    "   as Student General Misconduct (interfering with the teaching activities\n",
    "   of the University and/or inciting others to commit Academic Misconduct).\n",
    "   I understand that an allegation of Student General Misconduct may arise\n",
    "   regardless of whether or not I personally make use of such solutions\n",
    "   or sought benefit from such actions.\n",
    "\n",
    "   <b>Signed by</b>: Cheah Jia Huei 1078203\n",
    "   \n",
    "   <b>Dated</b>: 07/08/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 very expensive, 21 expensive, 54 affordable, 81 cheap\n",
    "\n",
    "# car_details = open('car.names','r').readlines()\n",
    "# car_details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb600fda251cf4f2b63845bfedaf16cb92e60656290d508957de2e5b2bb243fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
